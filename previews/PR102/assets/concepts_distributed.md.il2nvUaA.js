import{_ as a,C as s,o,c as i,aA as t,E as n,w as c,j as d}from"./chunks/framework.BdW1CY6Q.js";const P=JSON.parse('{"title":"Distributed","description":"","frontmatter":{},"headers":[],"relativePath":"concepts/distributed.md","filePath":"concepts/distributed.md","lastUpdated":null}'),l={name:"concepts/distributed.md"};function u(h,e,m,p,g,f){const r=s("center");return o(),i("div",null,[e[1]||(e[1]=t('<h1 id="distributed" tabindex="-1">Distributed <a class="header-anchor" href="#distributed" aria-label="Permalink to &quot;Distributed&quot;">​</a></h1><p><strong>Task-based parallelism</strong> in <a href="https://github.com/PTsolvers/Chmy.jl" target="_blank" rel="noreferrer">Chmy.jl</a> is featured by the usage of <a href="https://docs.julialang.org/en/v1/base/multi-threading/#Base.Threads.@spawn" target="_blank" rel="noreferrer"><code>Threads.@spawn</code></a>, with an additional layer of a <a href="./../developer_doc/workers">Worker</a> construct for efficiently managing the lifespan of tasks. Note that the task-based parallelism provides a high-level abstraction of program execution <strong>not only</strong> for <strong>shared-memory architecture</strong> on a single device, but it can be also extended to <strong>hybrid parallelism</strong>, consisting of both shared and distributed-memory parallelism. The <code>Distributed</code> module in Chmy.jl allows users to leverage the hybrid parallelism through the power of abstraction.</p><p>We will start with some basic background knowledge for understanding the architecture of modern HPC clusters, the underlying memory model and the programming paradigm complied with it.</p><h2 id="HPC-Cluster-and-Distributed-Memory" tabindex="-1">HPC Cluster &amp; Distributed Memory <a class="header-anchor" href="#HPC-Cluster-and-Distributed-Memory" aria-label="Permalink to &quot;HPC Cluster &amp;amp; Distributed Memory {#HPC-Cluster-and-Distributed-Memory}&quot;">​</a></h2><p>An <strong>high-performance computing (HPC)</strong> cluster is a collection of many separate <strong>servers (computers)</strong>, called <em>nodes</em>, which are connected via a fast interconnect. Each node manages its own private memory. Such system with interconnected nodes, without having access to memory of any other node, features the <strong>distributed memory model</strong>. The underlying fast interconnect (e.g. <em>InfiniBand</em>), that physically connects the nodes in the <strong>network</strong> via specialised hardware, can transfer the data from one node to another in an extremely efficient manner.</p>',5)),n(r,null,{default:c(()=>[...e[0]||(e[0]=[d("img",{src:"https://raw.githubusercontent.com/PTsolvers/Chmy.jl/main/docs/src/assets/compute_cluster.jpg",width:"35%"},null,-1)])]),_:1}),e[2]||(e[2]=t('<p>By using the fast interconnect, processes across different nodes can communicate with each other through the exchange of messages in a high-throughput, low-latency fashion. The syntax and semantics of how <strong>message passing</strong> should proceed through such network is defined by a standard called the <strong>Message-Passing Interface (MPI)</strong>, and there are different libraries that implement the standard, resulting in a wide range of choice (MPICH, Open MPI, MVAPICH etc.) for users. <a href="https://github.com/JuliaParallel/MPI.jl" target="_blank" rel="noreferrer">MPI.jl</a> package provides a high-level API for Julia users to call library routines of an implementation of user&#39;s choice.</p><div class="tip custom-block"><p class="custom-block-title">Message-Passing Interface (MPI) is a General Specification</p><p>In general, implementations based on <strong>MPI standard</strong> can be used for a great variety of computers, not just on HPC clusters, as long as these computers are connected by a communication network.</p></div><h2 id="Distributed-Architecture" tabindex="-1">Distributed Architecture <a class="header-anchor" href="#Distributed-Architecture" aria-label="Permalink to &quot;Distributed Architecture {#Distributed-Architecture}&quot;">​</a></h2><p>Expanding upon our understanding of message passing in HPC clusters, we now turn our focus to its application within GPU-enhanced environments in Chmy.jl. Our distributed architecture builds upon the abstraction of having GPU clusters that build on the same GPU architecture. Note that in general, GPU clusters may be equipped with hardware from different vendors, incorporating different types of GPUs to exploit their unique capabilities for specific tasks.</p><p>The <code>Distributed</code> architecture currently defaults to <a href="https://juliaparallel.org/MPI.jl/stable/usage/#CUDA-aware-MPI-support" target="_blank" rel="noreferrer">GPU-aware MPI</a> when the <code>CUDABackend</code> or <code>ROCBackend</code> GPU backends are selected for multi-GPU computations. For the <code>Distributed</code> architecture to function properly on those backends, a GPU-aware MPI library installation shall be used. Otherwise, a segmentation fault will occur.</p><p>The <code>Distributed</code> architecture can also be initialised to use CPU buffers making it compatible with non GPU-aware MPI library installations. This is the default behaviour for the <code>CPU</code> and <code>MetalBackend</code> backends. Setting the keyword argument <code>gpu_aware</code> allows to modified the default behaviour upon architecture initialisation.</p><div class="warning custom-block"><p class="custom-block-title">GPU-Aware MPI and Distributed Architecture</p><p>Using non GPU-aware MPI may require pinned (or page locked) memory buffers, a feature which is currently not implemented, and may result in slight performance reduction.</p></div>',7))])}const y=a(l,[["render",u]]);export{P as __pageData,y as default};
